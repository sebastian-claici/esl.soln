\section*{Chapter 2: Overview of Supervised Learning}

\paragraph{Statistical Decision Theory}\mbox{} \\

The goal is to minimize the expected prediction error:
\begin{align*}
  \mathrm{EPE}(f) &= \mathrm{E}\left(Y - f(X)\right)^2\\
                  &= \int [y - f(x)]^2 p(x, y) \d x \d y \numberthis \label{epe} 
\end{align*}

If we break down the expectation as $E_{X, Y} = E_{X} E_{Y|X=x}$ we can rewrite this as 
\begin{align*}
  \mathrm{EPE}(f) &= \E_X \E_{Y|X = x} (Y - f(X))^2\\
                  &= \int_X \int_{Y}[y - f(x)]^2 p(y | X = x)p(x) \d y \d x\\
                  &= \int_X p(x)\left(\int_Y [y - f(x)]^2 p(y | X = x)\d y \right)\d x
\end{align*}
We have moved the dependence on $p(x)$ outside the inner expectation. Since $f$ is unconstrained, we can solve for the optimal $f$ pointwise. That is:
\begin{align*}
  \argmin_{f} \mathrm{EPE}(f) = \argmin_{c} \int_Y [y - c]^2 p(y | X = x)\d y 
\end{align*}
Differentiating wrt $c$ and using the fact that
$$
\int_Y y\ p(y | X = x)\d y = \E(Y | X = x)
$$
gives us (2.13) in the book.

Nearest-neighbor methods try to model the regression function directly by averaging predictions around the query point $x$. To drive this point home, we can show that $\mathrm{NN}(x) \to x$ as the number of training points $N \to \infty$.

To sketch this proof out, assume $x_1, \ldots, x_N$ are drawn i.i.d from $X$. We want to bound $\min_i \|x - x_i\|$, but since this is a bit complicated, let's instead compute
$$
\probP (\|x - x_i\| \geq \epsilon, \forall i).
$$
for some $\epsilon > 0$.

Since the $x_i$ are sampled independently, we can expand the probability as 
$$
\probP (\|x - x_i\| \geq \epsilon, \forall i) = \prod_{i=1}^N \probP (\|x - x_i\| \geq \epsilon).
$$
As the $x_i$ are also identically distributed, the product can be written as 
$$
\left[\probP (\|x - x_i\| \geq \epsilon)\right]^N
$$
which goes to $0$ as $N\to \infty$ as long as the probability is not exactly $1$. This shows that with infinite samples the Nearest-neighbor of $x$ is $x$ and so nearest neighbors yields the Bayes optimal decision boundary even with a single neighbor.

However, we often do not have enough samples to use a model-free approach to regression. The second proposal is to assume the regression function is linear in its arguments:
$$
f(x) \approx x^T \beta
$$
If we plug this for $f$ into \eqref{epe}, we get
$$
\int [y - x^T \beta]^2 p(x, y) \d x \d y.
$$
We can differentiate this wrt $\beta$\footnote{See this \href{https://www.matrixcalculus.org/}{link} for a review of matrix calculus.}
\begin{align*}
  \frac{\partial \mathrm{EPE}}{\partial \beta} &= 2 \int x [y - x^T \beta] p(x, y) \d x \d y\\
                                               &= 2\left(\int x y\ p(x, y)\d x \d y - \int xx^T\beta\ p(x, y) \d x \d y\right)
\end{align*}
Since $\beta$ is not a random variable, we can set this to $0$ to arrive at the minimizer in (2.16) in the book:
$$
\beta = [\E (XX^T)]^{-1} \E (XY)
$$

\paragraph{Bias-Variance Decomposition}\mbox{} \\

We can express the mean-squared error in terms of a squared bias term and a variance term. In equation (2.25) in the book, these vary w.r.t. the training set $T$. To clarify the notation a bit, 
$x_0$ is the point $0$, $\hat{y}_0$ is the model estimate (in this case the nearest neighbor estimate), and $f(x_0)$ is the true value at $0$, but the following derivation holds generally for any model approximation $\hat{y}$ of a function $f(x)$\footnote{See, for example, the \href{https://en.wikipedia.org/wiki/Bias-variance_tradeoff}{wikipedia} page.}:

\begin{align*}
  \mathrm{MSE}(x_0) &= \E_{T}\left[f(x_0) - \hat{y}_0\right]^2\\
                    &= \E_{T}\left[\hat{y}_0 - \E_{T}[\hat{y}_0] + \E_{T}[\hat{y}_0] - f(x_0)]\right]^2\\
                    &= \E_{T}\left[\hat{y}_0 - \E_{T}[\hat{y}_0]\right]^2 + \left(f(x_0) - \E_{T}[\hat{y}_0]\right)^2\\
                    &= \mathrm{Var}_{T}(\hat{y}_0) + \mathrm{Bias}^2(\hat{y}_0)
\end{align*}
It is a somewhat instructive exercise to figure out how to go from the second line to the third. Easiest if you recall that
\begin{align*}
  \E_{T}[\E_{T}[y]] &= \E_{T}[y]\\
\E_{T}[f(x)] &= f(x)
\end{align*}

In the example in the book, the variance is consistently low, but the bias increases with dimension as the nearest point to $0$ becomes increasingly distant. 

We can discuss equations (2.27) and (2.28) in the book briefly. We have
$$
\hat{\beta} = (X^T X)^{-1} X^T y = (X^T X)^{-1}X^T (X \beta + \epsilon) = \beta + (X^T X)^{-1} X^T \epsilon
$$
and thus
$$
\hat{y}_0 = x_0^T \hat{\beta} = x_0^T (\beta + (X^T X)^{-1} X^T \epsilon)
$$
which gives
\begin{equation}
  \label{eqn:haty}
\hat{y}_0 = x_0^T \beta + \sum_{i=1}^N l_i(x_0)\epsilon_i
\end{equation}
since $x_0^T (X^T X)^{-1} X^T \epsilon$ is a scalar and 
$$
(x_0^T (X^T X)^{-1} X^T)^T = X(X^T X)^{-1}x_0
$$
to give the expression in the book.

Let's write out $\mathrm{EPE}(x_0)$. Note that because the true data was generated from a noisy process, we have to integrate out for $y_0$ given a fixed $x_0$:
\begin{align*}
  \mathrm{EPE}(x_0) &= \E_{y_0 | x_0} \E_{T}[y_0 - \hat{y}_0]^2
\end{align*}

In this particular case, because $Y$ depends on $X$ stochastically, $\E_{T} = \E_{X}\E_{Y | X}$

We will write out $y_0 - \hat{y}_0$ as:
\begin{align*}
  y_0 - \hat{y}_0 = (y_0 - x_0^T\beta) + (x_0^T\beta - \E_{T}[\hat{y}_0]) + (\E_{T}[\hat{y}_0] - \hat{y}_0)
\end{align*}
Let's square this, and keep in mind that $\E[\epsilon] = 0$, and $\V[\epsilon] = \sigma^2$:
\begin{align*}
  \E_{y_0 | x_0}\E_{T}[y_0 - \hat{y}_0]^2 =& \E_{y_0|x_0} [y_0 - x_0^T \beta]^2 + (x_0^T\beta - \E_{T}[\hat{y}_0])^2 + \E_{T}[\hat{y}_0 - \E_{T}[\hat{y}_0]]^2 +\\
                                           &\mathrm{cross\ terms}
\end{align*}
For the cross terms, we notice the following:
\begin{align*}
  \E_{y_0 | x_0} (y_0 - x_0^T\beta) &= 0\\
  \E_T (\E_T[\hat{y}_0] - \hat{y}_0) &= 0\\
\end{align*}
and
\begin{align*}
  (x_0^T\beta - \E_{T}[\hat{y}_0]) &= \E_{T} \left[\sum_{i=1}^N l_i(x_0) \epsilon_i \right]\\
                                   &= \E_{X} \left[\sum_{i=1}^N l_i(x)\E_{Y | X}(\epsilon_i)\right]\\
                                                  &= 0
\numberthis \label{eqn:zero-bias} 
\end{align*}
where we have used \eqref{eqn:haty}.

This gives
\begin{align*}
  \E_{y_0 | x_0}\E_{T}[y_0 - \hat{y}_0]^2 &= \E_{y_0|x_0} [y_0 - x_0^T \beta]^2 + (x_0^T\beta - \E_{T}[\hat{y}_0])^2 + \E_{T}[\hat{y}_0 - \E_{T}[\hat{y}_0]]^2 +\\
                                           &= \V[y_0 | x_0] + \mathrm{Bias}^2(\hat{y}_0) + \V_T(\hat{y}_0)
\end{align*}
but the bias is $0$ by \eqref{eqn:zero-bias}, and $\V[y_0 | x_0] = \sigma^2$, we have:
\begin{align*}
  \E_{y_0 | x_0}\E_{T}[y_0 - \hat{y}_0]^2 &= \sigma^2 + \V_{T}(\hat{y}_0).
\end{align*}
To finish the derivation, let's write out $\V_{T}(\hat{y}_0)$. We have just proved that $\E_{T}(\hat{y}_0) = x_0^T\beta$, and so
\begin{align*}
  \V_{T}(\hat{y}_0) &= \E_{T}\left[x_0^T(X^T X)^{-1} X^T \epsilon\right]^2\\
                    &= \E_{T}\left[x_0^T(X^T X)^{-1} X^T \epsilon\epsilon^T X (X^T X)^{-1} x_0\right]
\end{align*}
Since $\epsilon \sim N(0, \sigma^2)$, $\epsilon\epsilon^T = \sigma^2 I_N$, and we can replace above:
\begin{align*}
  \V_{T}(\hat{y}_0) &= \E_{T}\left[x_0^T(X^T X)^{-1} X^T X (X^T X)^{-1} x_0\right]\sigma^2\\
                    &= \E_{T}\left[x_0^T (X^T X)^{-1} x_0\right]\sigma^2
\end{align*}
which is the value in the book.

To derive (2.28), we assume large $N$ and that $X^T X \to N\mathrm{Cov}(X)$, hence:
\begin{align*}
  \E_{x_0}\mathrm{EPE}(x_0) &= \sigma^2 + \E_{x_0}\left[x_0^T (X^T X)^{-1} x_0\right]\sigma^2\\
                            &\sim \sigma^2 + \E_{x_0}\left[x_0^T \mathrm{Cov}(X)^{-1} x_0\right]\sigma^2/N
\numberthis \label{eqn:trace} 
\end{align*}
Now $x_0^T \mathrm{Cov}(X)^{-1} x_0$ is a scalar, and can be written as $\mathrm{trace}[x_0^T \mathrm{Cov}(X)^{-1} x_0]$. Exploiting the cyclic properties of the trace operator and its linearity (so we can move the expectation inside):
\begin{align*}
  \E_{x_0}\mathrm{trace}[x_0^T \mathrm{Cov}(X)^{-1} x_0] &= \E_{x_0}\mathrm{trace}[\mathrm{Cov}(X)^{-1} x_0 x_0^T] \\
                                                         &= \mathrm{trace}[\mathrm{Cov}(X)^{-1} \E_{x_0}(x_0 x_0^T)] \\
                                                         &= \mathrm{trace}[\mathrm{Cov}(X)^{-1} \mathrm{Cov}(x_0)]
\end{align*}
Since $\mathrm{Cov}(X) = \mathrm{Cov}(x_0)$, and since each training set point is $p$-dimensional:
\begin{align*}
  \mathrm{trace}[\mathrm{Cov}(X)^{-1} \mathrm{Cov}(x_0)] &= \mathrm{trace}[I_p] = p
\end{align*}
which when replaced in \eqref{eqn:trace} gives equation (2.28) in the book.

\subsection*{Exercises}

\exe{2.1. (Classifying to max)}
The wording here is a bit confusing, but I am assuming that $\hat{y}$ is constant, and we want to show 
$$
\argmin_k \|\hat{y} - t_k\| = \argmax_k \hat{y}_k.
$$

To prove this, note that $\|t_k\| = 1, \forall k$, and the length of $\hat{y}$ is constant with $0 \leq \hat{y}_k \leq 1$. For this setup, we can square the cost function without affecting the optimal value. Then
$$
\min_k \|\hat{y} - t_k\|^2 = \min_k \langle \hat{y} - t_k, \hat{y} - t_k \rangle = \|t_k\|^2 + \|\hat{y}\|^2 + 2\min_k (-\langle \hat{y}, t_k) \rangle
$$
Since $\min_k (-\langle \hat{y}, t_k)\rangle = \max_k \langle \hat{y}, t_k\rangle$ and $\langle \hat{y}, t_k\rangle = \hat{y}_k$ we have
$$
\argmin_k \|\hat{y} - t_k\|^2 =\argmax_k \langle \hat{y}, t_k \rangle = \argmax_k \hat{y}_k.
$$

\exe{2.2. (Bayes decision boundary)}
Computing the Bayes decision boundary only makes sense after the means of the two classes have been sampled. If we integrate out that step, the decision boundary is the diagonal through the origin that separates $[0, 1]^T$ and $[1, 0]^T$.

The procedure for sampling each data point looks something like this (for the \textcolor{blue}{blue} means):
\begin{align*}
(\mu_1, \ldots, \mu_{10}) &\sim \mathcal{N}([1, 0]^T, I_n)\\
\text{for i = 1..100}\\
k &\sim \mathrm{Cat}(10, [1/10, \ldots, 1/10])\\
x &\sim \mathcal{N}(\mu_k, I_n / 5)\\
\text{end for}
\end{align*}

The decision boundary is where
$$
\probP(\textcolor{blue}{\mathrm{blue}}) \sum_{i = 1}^{10} \exp\left\{-5\|x - \textcolor{blue}{\mu_k}\|^2 / 2\right\} = \probP(\textcolor{orange}{\mathrm{orange}}) \sum_{i = 1}^{10} \exp\left\{-5\|x - \textcolor{orange}{\nu_k}\|^2 / 2\right\} 
$$
which can be computed easily.

\exe{2.3. (Median distance in high dimensions)}
Let $x$ be a point sampled from the unit hypersphere. The probability that $\|x\| \leq r$ is equal to the ratio between the volume of the radius $r$ hypersphere and the volume of the unit sphere. That is, the cdf of $\|x\|$ in $d$-dimensions is
$$
F(r) = r^d.
$$

Now let's say we sample $n$ points $x_1, x_2, \ldots, x_n$. The probability that the closest of these is distance $r$ away can be written as
\begin{equation}
F(\min_i \|x\|_i \leq r) = 1 - \left(1 - F(r)\right)^n = 1 - (1 - r^d)^n
\label{eqn:hd-dist}
\end{equation}
since each of the $x_i$ is independent, and the event that the closest is distance $r$ away is the same as the event that all of them are at least distance $r$ away.

The median is defined as the distance $r$ for which $F(\min_i \|x\|_i \leq r) = 1 / 2$. If we substitute in \eqref{eqn:hd-dist}, we have
\begin{align*}
  1 - (1 - r^d)^n &= \frac{1}{2}\\
  1 - r^d &= \left(\frac{1}{2}\right)^{1 / n}\\
  r &= \left(1 - \left(\frac{1}{2}\right)^{1 / n}\right)^{1 / d}
\end{align*}

To compute the distance to the mean is a bit more of a challenge. Recall that the pdf of a random variable is the derivative of the cdf, and so, from \eqref{eqn:hd-dist} we have
$$
p(\min_i \|x\|_i \leq r) = nd(1 - r^d)^{n-1} r^{d - 1}
$$
The mean distance is found by integrating this over $r\in [0, 1]$:
$$
d_{\mathrm{mean}} = nd \int_0^1 (1 - r^d)^{n-1} r^{d - 1} \d r
$$
Integrating this explicitly is impossible, but we can relate it to the Beta function:
$$
\mathrm{B}(z_1, z_2) = \int_0^1 t^{z_1 - 1} (1 - t)^{z_2 - 1}\d t
$$
which can be integrated numerically and is available in most programming languages (e.g. \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.beta.html}{scipy.special.beta}).

\exe{2.4. (Average projection distance)}
The standard normal distribution is invariant to rotations, and so there is no difference between projecting on $a = x_0 / \|x_0\|$ and projecting on any of the axes. Since $\mathbf{x} \sim \mathcal{N}(0, I_p)$ is isotropic, each of the components of $\mathbf{x}$ are distributed as $\mathcal{N}(0, 1)$. Or, if you prefer a slightly different point of view, the joint distribution of $x_i \sim \mathcal{N}(0, 1)$ sampled independently is a $0$-mean, identity covariance Gaussian in $p$ dimensions.

We need to prove two related facts:
\begin{enumerate}
  \item{$\E\left[x_i^2\right] = 1, \forall i$}
  \item{$\E\left[\|x_0\|^2\right] = p$}
\end{enumerate}
The first item follows from the definition of the standard deviation of a Gaussian, but let's derive from first principles as a reminder. We want to integrate
\begin{align*}
  \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} x^2 e^{-\frac{x^2}{2}}\d x
\end{align*}
We can integrate this by parts with
\begin{align*}
  u &= x\\
  v &= -e^{-\frac{x^2}{2}}\\
  \d u &= \d x\\
  \d v &= xe^{-\frac{x^2}{2}}\d x
\end{align*}
and so
\begin{align*}
  \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} x^2 e^{-\frac{x^2}{2}}\d x &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} u \d v =\frac{1}{\sqrt{2\pi}} \left[uv\right]_{-\infty}^{\infty} - \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} v \d u\\
                                                                          &= \frac{1}{\sqrt{2\pi}} \left[-xe^{-\frac{x^2}{2}}\right]_{-\infty}^{\infty} + \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{x^2}{2}}\d x
\end{align*}
The first summand is $0$ since the exponential dominates the linear term and both limits are $0$. The second summand is related to the Gaussian integral after the transformation $t = x / \sqrt{2}$, $\d x = \sqrt{2}\d t$ which gives
\begin{align*}
  \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{x^2}{2}}\d x = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \sqrt{2} e^{-t^2}\d t = \frac{1}{\sqrt{2\pi}} \sqrt{2}\sqrt{\pi} = 1
\end{align*}
This proves $\E[x_i^2] = 1$. To show that the distance from any sample point to the origin is $p$, we can write out:
$$
\E\left[\|x_0\|^2\right] = \E\left[\sum_{i=1}^p x_i^2\right] = \sum_{i = 1}^p \E\left[x_i^2\right] = p
$$

\exe{2.5. (Bias-variance tradeoff for linear models)}
See main text for the derivation of the two equations.

\exe{2.6. (Repeated values and weighted least squares)}
