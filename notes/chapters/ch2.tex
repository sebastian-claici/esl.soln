\section*{Chapter 2: Overview of Supervised Learning}

\paragraph{Statistical Decision Theory}\mbox{} \\

The goal is to minimize the expected prediction error:
\begin{align*}
  \mathrm{EPE}(f) &= \mathrm{E}\left(Y - f(X)\right)^2\\
                  &= \int [y - f(x)]^2 p(x, y) \d x \d y \numberthis \label{epe} 
\end{align*}

If we break down the expectation as $E_{X, Y} = E_{X} E_{Y|X=x}$ we can rewrite this as 
\begin{align*}
  \mathrm{EPE}(f) &= \E_X \E_{Y|X = x} (Y - f(X))^2\\
                  &= \int_X \int_{Y}[y - f(x)]^2 p(y | X = x)p(x) \d y \d x\\
                  &= \int_X p(x)\left(\int_Y [y - f(x)]^2 p(y | X = x)\d y \right)\d x
\end{align*}
We have moved the dependence on $p(x)$ outside the inner expectation. Since $f$ is unconstrained, we can solve for the optimal $f$ pointwise. That is:
\begin{align*}
  \argmin_{f} \mathrm{EPE}(f) = \argmin_{c} \int_Y [y - c]^2 p(y | X = x)\d y 
\end{align*}
Differentiating wrt $c$ and using the fact that
$$
\int_Y y\ p(y | X = x)\d y = \E(Y | X = x)
$$
gives us (2.13) in the book.

Nearest-neighbor methods try to model the regression function directly by averaging predictions around the query point $x$. To drive this point home, we can show that $\mathrm{NN}(x) \to x$ as the number of training points $N \to \infty$.

To sketch this proof out, assume $x_1, \ldots, x_N$ are drawn i.i.d from $X$. We want to bound $\min_i \|x - x_i\|$, but since this is a bit complicated, let's instead compute
$$
\probP (\|x - x_i\| \geq \epsilon, \forall i).
$$
for some $\epsilon > 0$.

Since the $x_i$ are sampled independently, we can expand the probability as 
$$
\probP (\|x - x_i\| \geq \epsilon, \forall i) = \prod_{i=1}^N \probP (\|x - x_i\| \geq \epsilon).
$$
As the $x_i$ are also identically distributed, the product can be written as 
$$
\left[\probP (\|x - x_i\| \geq \epsilon)\right]^N
$$
which goes to $0$ as $N\to \infty$ as long as the probability is not exactly $1$. This shows that with infinite samples the Nearest-neighbor of $x$ is $x$ and so nearest neighbors yields the Bayes optimal decision boundary even with a single neighbor.

However, we often do not have enough samples to use a model-free approach to regression. The second proposal is to assume the regression function is linear in its arguments:
$$
f(x) \approx x^T \beta
$$
If we plug this for $f$ into \eqref{epe}, we get
$$
\int [y - x^T \beta]^2 p(x, y) \d x \d y.
$$
We can differentiate this wrt $\beta$\footnote{See this \href{https://www.matrixcalculus.org/}{link} for a review of matrix calculus.}
\begin{align*}
  \frac{\partial \mathrm{EPE}}{\partial \beta} &= 2 \int x [y - x^T \beta] p(x, y) \d x \d y\\
                                               &= 2\left(\int x y\ p(x, y)\d x \d y - \int xx^T\beta\ p(x, y) \d x \d y\right)
\end{align*}
Since $\beta$ is not a random variable, we can set this to $0$ to arrive at the minimizer in (2.16) in the book:
$$
\beta = [\E (XX^T)]^{-1} \E (XY)
$$

\paragraph{Bias-Variance Decomposition}\mbox{} \\

We can express the mean-squared error in terms of a squared bias term and a variance term. In equation (2.25) in the book, these vary w.r.t. the training set $T$. To clarify the notation a bit, 
$x_0$ is the point $0$, $\hat{y}_0$ is the model estimate (in this case the nearest neighbor estimate), and $f(x_0)$ is the true value at $0$, but the following derivation holds generally for any model approximation $\hat{y}$ of a function $f(x)$\footnote{See, for example, the \href{https://en.wikipedia.org/wiki/Bias-variance_tradeoff}{wikipedia} page.}:

\begin{align*}
  \mathrm{MSE}(x_0) &= \E_{T}\left[f(x_0) - \hat{y}_0\right]^2\\
                    &= \E_{T}\left[\hat{y}_0 - \E_{T}[\hat{y}_0] + \E_{T}[\hat{y}_0] - f(x_0)]\right]^2\\
                    &= \E_{T}\left[\hat{y}_0 - \E_{T}[\hat{y}_0]\right]^2 + \left(f(x_0) - \E_{T}[\hat{y}_0]\right)^2\\
                    &= \mathrm{Var}_{T}(\hat{y}_0) + \mathrm{Bias}^2(\hat{y}_0)
\end{align*}
It is a somewhat instructive exercise to figure out how to go from the second line to the third. Easiest if you recall that
\begin{align*}
  \E_{T}[\E_{T}[y]] &= \E_{T}[y]\\
\E_{T}[f(x)] &= f(x)
\end{align*}

In the example in the book, the variance is consistently low, but the bias increases with dimension as the nearest point to $0$ becomes increasingly distant. 

We can discuss equations (2.27) and (2.28) in the book briefly. We have
$$
\hat{\beta} = (X^T X)^{-1} X^T y = (X^T X)^{-1}X^T (X \beta + \epsilon) = \beta + (X^T X)^{-1} X^T \epsilon
$$
and thus
$$
\hat{y}_0 = x_0^T \hat{\beta} = x_0^T (\beta + (X^T X)^{-1} X^T \epsilon)
$$
which gives
$$
\hat{y}_0 = x_0^T \beta + \sum_{i=1}^N l_i(x_0)\epsilon_i
$$
since $x_0^T (X^T X)^{-1} X^T \epsilon$ is a scalar and 
$$
(x_0^T (X^T X)^{-1} X^T)^T = X(X^T X)^{-1}x_0
$$
to give the expression in the book.

Let's write out $\mathrm{EPE}(x_0)$:
\begin{align*}
\end{align*}

\subsection*{Exercises}
