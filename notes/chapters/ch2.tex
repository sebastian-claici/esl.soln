\section*{Chapter 2: Overview of Supervised Learning}

\paragraph{Statistical Decision Theory}\mbox{} \\

The goal is to minimize the expected prediction error:
\begin{align*}
  \mathrm{EPE}(f) &= \mathrm{E}\left(Y - f(X)\right)^2\\
                  &= \int [y - f(x)]^2 p(x, y) \d x \d y \numberthis \label{epe} 
\end{align*}

If we break down the expectation as $E_{X, Y} = E_{X} E_{Y|X=x}$ we can rewrite this as 
\begin{align*}
  \mathrm{EPE}(f) &= \E_X \E_{Y|X = x} (Y - f(X))^2\\
                  &= \int_X \int_{Y}[y - f(x)]^2 p(y | X = x)p(x) \d y \d x\\
                  &= \int_X p(x)\left(\int_Y [y - f(x)]^2 p(y | X = x)\d y \right)\d x
\end{align*}
We have moved the dependence on $p(x)$ outside the inner expectation. Since $f$ is unconstrained, we can solve for the optimal $f$ pointwise. That is:
\begin{align*}
  \argmin_{f} \mathrm{EPE}(f) = \argmin_{c} \int_Y [y - c]^2 p(y | X = x)\d y 
\end{align*}
Differentiating wrt $c$ and using the fact that
$$
\int_Y y\ p(y | X = x)\d y = \E(Y | X = x)
$$
gives us (2.13) in the book.

Nearest-neighbor methods try to model the regression function directly by averaging predictions around the query point $x$. To drive this point home, we can show that $\mathrm{NN}(x) \to x$ as the number of training points $N \to \infty$.

To sketch this proof out, assume $x_1, \ldots, x_N$ are drawn i.i.d from $X$. We want to bound $\min_i \|x - x_i\|$, but since this is a bit complicated, let's instead compute
$$
\probP (\|x - x_i\| \geq \epsilon, \forall i).
$$
for some $\epsilon > 0$.

Since the $x_i$ are sampled independently, we can expand the probability as 
$$
\probP (\|x - x_i\| \geq \epsilon, \forall i) = \prod_{i=1}^N \probP (\|x - x_i\| \geq \epsilon).
$$
As the $x_i$ are also identically distributed, the product can be written as 
$$
\left[\probP (\|x - x_i\| \geq \epsilon)\right]^N
$$
which goes to $0$ as $N\to \infty$ as long as the probability is not exactly $1$. This shows that with infinite samples the Nearest-neighbor of $x$ is $x$ and so nearest neighbors yields the Bayes optimal decision boundary even with a single neighbor.

However, we often do not have enough samples to use a model-free approach to regression. The second proposal is to assume the regression function is linear in its arguments:
$$
f(x) \approx x^T \beta
$$
If we plug this for $f$ into \eqref{epe}, we get
$$
\int [y - x^T \beta]^2 p(x, y) \d x \d y.
$$
We can differentiate this wrt $\beta$\footnote{See this \href{https://www.matrixcalculus.org/}{link} for a review of matrix calculus.}
\begin{align*}
  \frac{\partial \mathrm{EPE}}{\partial \beta} &= 2 \int x [y - x^T \beta] p(x, y) \d x \d y\\
                                               &= 2\left(\int x y\ p(x, y)\d x \d y - \int xx^T\beta\ p(x, y) \d x \d y\right)
\end{align*}
Since $\beta$ is not a random variable, we can set this to $0$ to arrive at the minimizer in (2.16) in the book:
$$
\beta = [\E (XX^T)]^{-1} \E (XY)
$$
